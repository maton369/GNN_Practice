{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b35cdbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch の基本モジュールをインポート\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# GATConv: グラフ注意ネットワーク（GAT）のための畳み込みレイヤー\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "# Planetoid: Cora, Citeseer, Pubmed などのベンチマーク用グラフデータセットを提供するモジュール\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1799ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planetoid データセット（ここでは 'Cora'）をダウンロードし、指定パスに保存する\n",
    "# 'root' は保存先ディレクトリ、'name' は使用するデータセット名を指定\n",
    "dataset: Planetoid = Planetoid(root=\"/tmp/Cora\", name=\"Cora\")\n",
    "\n",
    "# データセットは1つの大きなグラフで構成されているため、インデックス 0 で取り出す\n",
    "data = dataset[\n",
    "    0\n",
    "]  # data は1つのグラフデータを保持しており、x, edge_index, y などの属性を持つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d39d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# グラフ注意ネットワーク（GAT）の定義\n",
    "class GAT(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_d: int, mid_d: int, out_d: int, heads: int, dropout: float = 0.6\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        GAT モデルの初期化\n",
    "\n",
    "        Parameters:\n",
    "        - in_d (int): 入力特徴量の次元数（ノード特徴量の次元）\n",
    "        - mid_d (int): 中間層の出力次元数（各ヘッドの出力次元）\n",
    "        - out_d (int): 出力層の次元数（分類クラス数など）\n",
    "        - heads (int): 注意ヘッドの数（マルチヘッドアテンション）\n",
    "        - dropout (float): ドロップアウト率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 第1層: GATConv（マルチヘッドアテンションあり）\n",
    "        self.conv1 = GATConv(in_d, mid_d, heads=heads, dropout=dropout)\n",
    "        # 第2層: GATConv（ヘッド統合後の次元数を入力とする）\n",
    "        self.conv2 = GATConv(mid_d * heads, out_d, dropout=dropout)\n",
    "\n",
    "    def forward(self, data: Data) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        フォワードパス\n",
    "\n",
    "        Parameters:\n",
    "        - data (Data): PyG の Data オブジェクト（x, edge_index などを含む）\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: 出力ロジット（対数ソフトマックス済み）\n",
    "        \"\"\"\n",
    "        # ノードの特徴量とエッジ情報を取得\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # 第1層 GATConv ＋ 活性化関数（ELU）\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # 第2層 GATConv（出力）\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # クラスごとの対数確率を出力\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5839f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GATモデルのインスタンス化\n",
    "# 入力次元: 各ノードの特徴量の次元（dataset.num_node_features）\n",
    "# 中間層の出力次元: 各ヘッドにつき8次元\n",
    "# 出力次元: クラス数（dataset.num_classes）\n",
    "# ヘッド数: 8（マルチヘッドアテンション）\n",
    "model = GAT(dataset.num_node_features, 8, dataset.num_classes, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e06d709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法として確率的勾配降下法（SGD）を設定\n",
    "# - model.parameters(): 学習対象のGATモデルのパラメータ群\n",
    "# - lr=0.1: 学習率（各ステップでのパラメータの更新量）\n",
    "# - weight_decay=1e-4: L2正則化（過学習防止のための項）\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c0e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch: int) -> None:\n",
    "    \"\"\"\n",
    "    GATモデルを指定されたエポック数だけ学習する関数。\n",
    "\n",
    "    Parameters:\n",
    "    epoch (int): 学習を繰り返すエポック数\n",
    "    \"\"\"\n",
    "    model.train()  # モデルを学習モードに設定（DropoutやBatchNormが有効になる）\n",
    "\n",
    "    for i in range(epoch):\n",
    "        optimizer.zero_grad()  # 勾配の初期化（前回のバックプロパゲーションの勾配をリセット）\n",
    "\n",
    "        out = model(data)  # 入力データに対する予測（各ノードのクラスごとの確率）\n",
    "\n",
    "        # 負の対数尤度損失（学習用マスクがTrueのノードだけを対象に計算）\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "        loss.backward()  # 誤差逆伝播による勾配計算\n",
    "        optimizer.step()  # モデルのパラメータを更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b359004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.85 s, sys: 3.97 s, total: 13.8 s\n",
      "Wall time: 8.18 s\n"
     ]
    }
   ],
   "source": [
    "%time train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "406e81f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.807"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルを評価モードに設定（DropoutやBatchNormを無効にする）\n",
    "model.eval()\n",
    "\n",
    "# 各ノードに対する予測ラベルを取得（クラスごとの確率の最大値のインデックスを取得）\n",
    "pred: torch.Tensor = model(data).argmax(dim=1)\n",
    "\n",
    "# テストデータにおける正解数をカウント\n",
    "correct: torch.Tensor = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "\n",
    "# テストデータ全体に対する正解率（accuracy）を計算\n",
    "acc: float = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "# 結果（正解率）を表示\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
